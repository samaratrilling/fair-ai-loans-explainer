{"Title":"Equally Right for Everyone\r\nA More Fair Way For Algorithms to Grant Housing Loans","Slide1":"Once Upon a Time, Housing Loans Were Unfair\r\n\r\n\r\nOnce upon a time, women got fewer housing loans than men did, and they needed to have a higher credit score in order to qualify for the same loan.\r\n\r\n\r\nWhy is this?\r\nHistorically, bank lenders met with people in person to decide on loans. \r\nBank lenders might have looked at a woman and thought she might have a lower income than a man would. Or that she’s going to leave her job when she has a baby, and hence would have lower household income.\r\nMaybe the lender didn’t know they were doing it, but just “felt” like men were more trustworthy.\r\n\r\n\r\nWhatever the reason, this study [cite] found that on average women pay more for loans than men do - so banks think they’re riskier - but women tend to pay loans back more often than men.\r\n\r\n\r\nThis was super unfair, and also hard to detect - you had to visit the bank itself and say “why did you deny this woman a loan?” People often wouldn’t say outright that it was because the applicant was a woman.\r\nAnd if you wanted to change loan officers’ behavior, you had to go to every lender at every bank, tell them how to do it better, and check up on them to make sure they were doing it. It took a lot of time, and people didn’t like to change that much.\r\n\r\n\r\nSo things mostly stayed the same.\r\nThen, computers happened.","Slide2":"Algorithms Currently Don’t Help. But They Could.\r\nIf computers couldn’t see people, and we didn’t tell them what gender a person was, you’d think they’d automatically be totally fair. But it turns out they still discriminate.\r\n\r\n\r\n[stat - algorithms discriminate 40% less, but they still discriminate]\r\n\r\n\r\nWhy is this?\r\n\r\n\r\nWomen tend to take out fewer housing loans than men do [stat]. Women make less money on average [stat - gender pay gap], and they’re more likely to live in areas with fewer banks [stat]. When they do take out loans, \r\n\r\n\r\nSo there’s less data overall - less data on when women pay back loans and less data on when they don’t pay back loans.\r\n\r\n\r\nSince banks have less data to go on, banks make more mistakes in giving women loans - and more frequently, the mistake is that the bank should have given them a loan and they don’t.\r\n\r\n\r\nThat goes to show that the models aren’t very good at predicting risk for women.","Slide3":"Algorithms Could Be the Solution\r\n\r\n\r\nCurrently we start by assuming we should give men and women the same number of loans, if they’re equally creditworthy. But we allow lenders to break that rule if they have a good reason to - and there are lots of things that count as good reasons, and we don't have a rule about how much they can break that rule by.\r\n\r\n\r\nSo we end up with a system that actually isn’t fair - it still lends more to men. And it’s not very specific - we need lawyers to argue whether something counts as a good reason. If models change every week, we need to re-evaluate that model to make sure it’s fair super fast - not go to court each time it changes.\r\n\r\n\r\nHow can we fix this?","Slide4":"A Better Way To Be Fair\r\n\r\n\r\nWith algorithms, we can take advantage of the fact that we can ask an algorithm what it's going to do before it does it.\r\n\r\n\r\nSo we can ask an algorithm, here's a person who applied for a loan 10 years ago - we know if they paid their loan back, but the algorithm doesn't know. And we ask, Would you give them a loan?\r\n\r\n\r\nAnd we count how many people the algorithm gets right.","Slide5":"Now if the algorithm is really good at guessing for everyone, there are no problems - the bank makes a lot of money, everyone who should get a loan gets a loan, and we don’t lend to anyone who won’t pay back their loans.\r\n\r\n\r\nBut in most cases, the algorithm is better at guessing for one group than another. \r\nUsually we have more data on one group- maybe one group applies for loans more often, so the algorithm has seen people like them more often and is more sure what to do with them.\r\n\r\n\r\nIf one of those groups generally doesn’t apply for loans as often - maybe they make less money, maybe they live in areas of town where there aren’t as many banks, that means the algorithm will be worse at predicting whether those people will pay back their loans.\r\n\r\n\r\nAnd that means you’re going to have one group who gets fewer loans than the other group, even if they would have paid them back.\r\n\r\n\r\nThat’s unfair.","Slide6":"\r\n\r\nFirst, we might say if we don’t know how to treat someone, we’ll err on the side of giving them the benefit of the doubt and give them the loan. We generally like this - more people overall get loans.\r\nBut this means that if we know more about one group, we can more accurately deny them loans. In this case, we know more about men, so we can more accurately deny them loans, but we don’t know as much about women, so we mess up and accidentally give them more loans than we should.\r\n\r\n\r\nYou might think this mostly just hurts men.\r\nBut actually, this also hurts women. If you give lots of women loans who might not be able to pay them back, their credit scores are going to go down and they’ll end up with lower average credit scores than the other group, which isn’t fair either. [link to ML Fairness Gym]\r\n\r\n\r\nSo this fix [equal opportunity] doesn’t do what we want it to.","Slide7":"Equalized Odds\r\nEqualized odds means we’re going to make the same number of mistakes and the same kind of mistakes for each group. If I accidentally overlend to one group, I have to overlend to the other. If I underlend to one group, I have to underlend to the other.\r\n\r\n\r\nAnd what this incentivizes is for me to not underlend or overlend for anyone. I want to get as good as possible at predicting the risk of default for everyone, and I want to get equally good at it for all groups.","Slide8":"What are the downsides?\r\n\r\n\r\n If I’m equally good at predicting for men and women, then there are no problems. But if I’m much better at predicting for men than women, I might have to do a lot of work to fix my model.\r\n\r\n\r\nIf I’m not very good at predicting for women, then I can’t predict well for men either, and I’ll lose money.\r\nTo fix it, I can either get worse at predicting men (easy, but not what I want) or I can get better at predicting women (harder).","Slide9":"Equalized Odds: Fixing Your Model to be Fairer\r\n\r\n\r\n1. Get more training data for women. Get more examples of times people like them paid off mortgages or didn’t pay them back.\r\n2. Try to find different evidence that women are creditworthy - like if they paid their rent, or utility bills, or paid back a short term loan, try taking that into account. That now gives a new signal that makes it easier to predict whether someone from that group would pay a loan back.\r\n3. \r\n\r\nMany companies specialize in giving suggestions like this to make models more fair. With the right fairness goal in place, you can now be as creative as you want in deciding how to get there [cite - ZestFinance, BLDS, etc. cite paper on mitigation techniques]","Conclusion":"\r\n\r\nThis may sound hard.\r\n\r\n\r\nBut with equalized odds, the incentives are aligned - banks and customers both want more accuracy in lending. By requiring banks to make the same number of mistakes for each group, we give them a financial reason to get better at predicting whether women will pay back their loans. The fewer mistakes a bank makes, the more money they can make. And the fewer mistakes a bank makes, the more money they have on hand to give more loans - to everyone."}